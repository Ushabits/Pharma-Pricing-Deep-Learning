{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c192a810-6a11-40a6-b673-fe72ec8ac3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Starting training (Classification First)...\n",
      "Epoch 1/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 7ms/step - loss: 0.9473 - price_output_loss: 1.0564 - price_output_mae: 0.7236 - tier_output_accuracy: 0.5750 - tier_output_loss: 0.8945 - val_loss: 0.7566 - val_price_output_loss: 0.6348 - val_price_output_mae: 0.5402 - val_tier_output_accuracy: 0.6757 - val_tier_output_loss: 0.7255 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - loss: 0.7370 - price_output_loss: 0.5904 - price_output_mae: 0.5134 - tier_output_accuracy: 0.6909 - tier_output_loss: 0.7075 - val_loss: 0.6789 - val_price_output_loss: 0.5389 - val_price_output_mae: 0.4772 - val_tier_output_accuracy: 0.7169 - val_tier_output_loss: 0.6532 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - loss: 0.6716 - price_output_loss: 0.5300 - price_output_mae: 0.4774 - tier_output_accuracy: 0.7246 - tier_output_loss: 0.6451 - val_loss: 0.6529 - val_price_output_loss: 0.4751 - val_price_output_mae: 0.4377 - val_tier_output_accuracy: 0.7289 - val_tier_output_loss: 0.6299 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - loss: 0.6286 - price_output_loss: 0.4984 - price_output_mae: 0.4595 - tier_output_accuracy: 0.7436 - tier_output_loss: 0.6036 - val_loss: 0.6541 - val_price_output_loss: 0.4961 - val_price_output_mae: 0.4511 - val_tier_output_accuracy: 0.7288 - val_tier_output_loss: 0.6298 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - loss: 0.6003 - price_output_loss: 0.4783 - price_output_mae: 0.4481 - tier_output_accuracy: 0.7583 - tier_output_loss: 0.5763 - val_loss: 0.6343 - val_price_output_loss: 0.4613 - val_price_output_mae: 0.4301 - val_tier_output_accuracy: 0.7392 - val_tier_output_loss: 0.6117 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - loss: 0.5770 - price_output_loss: 0.4599 - price_output_mae: 0.4388 - tier_output_accuracy: 0.7683 - tier_output_loss: 0.5540 - val_loss: 0.6378 - val_price_output_loss: 0.4474 - val_price_output_mae: 0.4245 - val_tier_output_accuracy: 0.7384 - val_tier_output_loss: 0.6158 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - loss: 0.5597 - price_output_loss: 0.4517 - price_output_mae: 0.4334 - tier_output_accuracy: 0.7769 - tier_output_loss: 0.5371 - val_loss: 0.6331 - val_price_output_loss: 0.4465 - val_price_output_mae: 0.4299 - val_tier_output_accuracy: 0.7410 - val_tier_output_loss: 0.6111 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - loss: 0.5457 - price_output_loss: 0.4388 - price_output_mae: 0.4270 - tier_output_accuracy: 0.7835 - tier_output_loss: 0.5237 - val_loss: 0.6457 - val_price_output_loss: 0.4364 - val_price_output_mae: 0.4223 - val_tier_output_accuracy: 0.7408 - val_tier_output_loss: 0.6239 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 0.5321 - price_output_loss: 0.4288 - price_output_mae: 0.4222 - tier_output_accuracy: 0.7877 - tier_output_loss: 0.5107 - val_loss: 0.6583 - val_price_output_loss: 0.4373 - val_price_output_mae: 0.4252 - val_tier_output_accuracy: 0.7386 - val_tier_output_loss: 0.6363 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 0.5214 - price_output_loss: 0.4187 - price_output_mae: 0.4180 - tier_output_accuracy: 0.7925 - tier_output_loss: 0.5004 - val_loss: 0.6483 - val_price_output_loss: 0.4294 - val_price_output_mae: 0.4190 - val_tier_output_accuracy: 0.7410 - val_tier_output_loss: 0.6269 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 0.5143 - price_output_loss: 0.4156 - price_output_mae: 0.4153 - tier_output_accuracy: 0.7955 - tier_output_loss: 0.4935 - val_loss: 0.6501 - val_price_output_loss: 0.4326 - val_price_output_mae: 0.4200 - val_tier_output_accuracy: 0.7417 - val_tier_output_loss: 0.6282 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - loss: 0.5052 - price_output_loss: 0.4097 - price_output_mae: 0.4129 - tier_output_accuracy: 0.7994 - tier_output_loss: 0.4846 - val_loss: 0.6495 - val_price_output_loss: 0.4197 - val_price_output_mae: 0.4144 - val_tier_output_accuracy: 0.7461 - val_tier_output_loss: 0.6287 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - loss: 0.4983 - price_output_loss: 0.4018 - price_output_mae: 0.4092 - tier_output_accuracy: 0.8028 - tier_output_loss: 0.4782 - val_loss: 0.6477 - val_price_output_loss: 0.4134 - val_price_output_mae: 0.4135 - val_tier_output_accuracy: 0.7483 - val_tier_output_loss: 0.6267 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 0.4937 - price_output_loss: 0.4009 - price_output_mae: 0.4076 - tier_output_accuracy: 0.8049 - tier_output_loss: 0.4736 - val_loss: 0.6531 - val_price_output_loss: 0.4090 - val_price_output_mae: 0.4096 - val_tier_output_accuracy: 0.7480 - val_tier_output_loss: 0.6328 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - loss: 0.4876 - price_output_loss: 0.3936 - price_output_mae: 0.4042 - tier_output_accuracy: 0.8066 - tier_output_loss: 0.4679 - val_loss: 0.6460 - val_price_output_loss: 0.4061 - val_price_output_mae: 0.4091 - val_tier_output_accuracy: 0.7476 - val_tier_output_loss: 0.6256 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 0.4828 - price_output_loss: 0.3906 - price_output_mae: 0.4026 - tier_output_accuracy: 0.8099 - tier_output_loss: 0.4632 - val_loss: 0.6493 - val_price_output_loss: 0.4163 - val_price_output_mae: 0.4139 - val_tier_output_accuracy: 0.7492 - val_tier_output_loss: 0.6284 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 0.4773 - price_output_loss: 0.3864 - price_output_mae: 0.4004 - tier_output_accuracy: 0.8116 - tier_output_loss: 0.4579 - val_loss: 0.6745 - val_price_output_loss: 0.4111 - val_price_output_mae: 0.4156 - val_tier_output_accuracy: 0.7473 - val_tier_output_loss: 0.6535 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 0.4715 - price_output_loss: 0.3829 - price_output_mae: 0.3987 - tier_output_accuracy: 0.8139 - tier_output_loss: 0.4523 - val_loss: 0.6473 - val_price_output_loss: 0.4075 - val_price_output_mae: 0.4140 - val_tier_output_accuracy: 0.7499 - val_tier_output_loss: 0.6270 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 0.4690 - price_output_loss: 0.3805 - price_output_mae: 0.3975 - tier_output_accuracy: 0.8144 - tier_output_loss: 0.4500 - val_loss: 0.6542 - val_price_output_loss: 0.4020 - val_price_output_mae: 0.4089 - val_tier_output_accuracy: 0.7506 - val_tier_output_loss: 0.6346 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 0.4663 - price_output_loss: 0.3770 - price_output_mae: 0.3953 - tier_output_accuracy: 0.8152 - tier_output_loss: 0.4474 - val_loss: 0.6434 - val_price_output_loss: 0.4108 - val_price_output_mae: 0.4147 - val_tier_output_accuracy: 0.7536 - val_tier_output_loss: 0.6230 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 0.4606 - price_output_loss: 0.3714 - price_output_mae: 0.3935 - tier_output_accuracy: 0.8181 - tier_output_loss: 0.4420 - val_loss: 0.6564 - val_price_output_loss: 0.4114 - val_price_output_mae: 0.4196 - val_tier_output_accuracy: 0.7495 - val_tier_output_loss: 0.6361 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 6ms/step - loss: 0.4573 - price_output_loss: 0.3707 - price_output_mae: 0.3920 - tier_output_accuracy: 0.8205 - tier_output_loss: 0.4388 - val_loss: 0.6536 - val_price_output_loss: 0.3981 - val_price_output_mae: 0.4114 - val_tier_output_accuracy: 0.7536 - val_tier_output_loss: 0.6337 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 0.4556 - price_output_loss: 0.3695 - price_output_mae: 0.3915 - tier_output_accuracy: 0.8200 - tier_output_loss: 0.4371 - val_loss: 0.6529 - val_price_output_loss: 0.3986 - val_price_output_mae: 0.4104 - val_tier_output_accuracy: 0.7552 - val_tier_output_loss: 0.6330 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 0.4521 - price_output_loss: 0.3646 - price_output_mae: 0.3896 - tier_output_accuracy: 0.8216 - tier_output_loss: 0.4338 - val_loss: 0.6521 - val_price_output_loss: 0.4006 - val_price_output_mae: 0.4172 - val_tier_output_accuracy: 0.7515 - val_tier_output_loss: 0.6322 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 0.4483 - price_output_loss: 0.3643 - price_output_mae: 0.3886 - tier_output_accuracy: 0.8235 - tier_output_loss: 0.4301 - val_loss: 0.6634 - val_price_output_loss: 0.3938 - val_price_output_mae: 0.4122 - val_tier_output_accuracy: 0.7509 - val_tier_output_loss: 0.6440 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 0.4470 - price_output_loss: 0.3639 - price_output_mae: 0.3885 - tier_output_accuracy: 0.8247 - tier_output_loss: 0.4287 - val_loss: 0.6612 - val_price_output_loss: 0.4008 - val_price_output_mae: 0.4146 - val_tier_output_accuracy: 0.7526 - val_tier_output_loss: 0.6416 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m2973/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4399 - price_output_loss: 0.3603 - price_output_mae: 0.3875 - tier_output_accuracy: 0.8261 - tier_output_loss: 0.4219\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - loss: 0.4414 - price_output_loss: 0.3594 - price_output_mae: 0.3865 - tier_output_accuracy: 0.8256 - tier_output_loss: 0.4234 - val_loss: 0.6530 - val_price_output_loss: 0.4006 - val_price_output_mae: 0.4159 - val_tier_output_accuracy: 0.7525 - val_tier_output_loss: 0.6329 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - loss: 0.4269 - price_output_loss: 0.3430 - price_output_mae: 0.3757 - tier_output_accuracy: 0.8320 - tier_output_loss: 0.4098 - val_loss: 0.6360 - val_price_output_loss: 0.3659 - val_price_output_mae: 0.3897 - val_tier_output_accuracy: 0.7631 - val_tier_output_loss: 0.6178 - learning_rate: 5.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 0.4156 - price_output_loss: 0.3385 - price_output_mae: 0.3733 - tier_output_accuracy: 0.8374 - tier_output_loss: 0.3986 - val_loss: 0.6365 - val_price_output_loss: 0.3642 - val_price_output_mae: 0.3886 - val_tier_output_accuracy: 0.7637 - val_tier_output_loss: 0.6186 - learning_rate: 5.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 0.4084 - price_output_loss: 0.3355 - price_output_mae: 0.3718 - tier_output_accuracy: 0.8402 - tier_output_loss: 0.3916 - val_loss: 0.6375 - val_price_output_loss: 0.3667 - val_price_output_mae: 0.3895 - val_tier_output_accuracy: 0.7650 - val_tier_output_loss: 0.6197 - learning_rate: 5.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - loss: 0.4037 - price_output_loss: 0.3333 - price_output_mae: 0.3709 - tier_output_accuracy: 0.8420 - tier_output_loss: 0.3871 - val_loss: 0.6419 - val_price_output_loss: 0.3666 - val_price_output_mae: 0.3893 - val_tier_output_accuracy: 0.7650 - val_tier_output_loss: 0.6241 - learning_rate: 5.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - loss: 0.4031 - price_output_loss: 0.3338 - price_output_mae: 0.3706 - tier_output_accuracy: 0.8422 - tier_output_loss: 0.3864 - val_loss: 0.6419 - val_price_output_loss: 0.3638 - val_price_output_mae: 0.3887 - val_tier_output_accuracy: 0.7627 - val_tier_output_loss: 0.6245 - learning_rate: 5.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 0.4004 - price_output_loss: 0.3322 - price_output_mae: 0.3701 - tier_output_accuracy: 0.8434 - tier_output_loss: 0.3837 - val_loss: 0.6403 - val_price_output_loss: 0.3652 - val_price_output_mae: 0.3883 - val_tier_output_accuracy: 0.7675 - val_tier_output_loss: 0.6226 - learning_rate: 5.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 0.3974 - price_output_loss: 0.3293 - price_output_mae: 0.3692 - tier_output_accuracy: 0.8449 - tier_output_loss: 0.3809 - val_loss: 0.6495 - val_price_output_loss: 0.3628 - val_price_output_mae: 0.3875 - val_tier_output_accuracy: 0.7625 - val_tier_output_loss: 0.6318 - learning_rate: 5.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - loss: 0.3953 - price_output_loss: 0.3298 - price_output_mae: 0.3693 - tier_output_accuracy: 0.8454 - tier_output_loss: 0.3787 - val_loss: 0.6491 - val_price_output_loss: 0.3611 - val_price_output_mae: 0.3886 - val_tier_output_accuracy: 0.7639 - val_tier_output_loss: 0.6321 - learning_rate: 5.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7ms/step - loss: 0.3927 - price_output_loss: 0.3282 - price_output_mae: 0.3683 - tier_output_accuracy: 0.8457 - tier_output_loss: 0.3762 - val_loss: 0.6620 - val_price_output_loss: 0.3622 - val_price_output_mae: 0.3900 - val_tier_output_accuracy: 0.7630 - val_tier_output_loss: 0.6441 - learning_rate: 5.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m2970/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3898 - price_output_loss: 0.3307 - price_output_mae: 0.3694 - tier_output_accuracy: 0.8473 - tier_output_loss: 0.3732\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 6ms/step - loss: 0.3906 - price_output_loss: 0.3263 - price_output_mae: 0.3680 - tier_output_accuracy: 0.8464 - tier_output_loss: 0.3742 - val_loss: 0.6532 - val_price_output_loss: 0.3632 - val_price_output_mae: 0.3910 - val_tier_output_accuracy: 0.7645 - val_tier_output_loss: 0.6351 - learning_rate: 5.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 0.3869 - price_output_loss: 0.3208 - price_output_mae: 0.3632 - tier_output_accuracy: 0.8482 - tier_output_loss: 0.3708 - val_loss: 0.6419 - val_price_output_loss: 0.3565 - val_price_output_mae: 0.3853 - val_tier_output_accuracy: 0.7694 - val_tier_output_loss: 0.6243 - learning_rate: 2.5000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 0.3777 - price_output_loss: 0.3160 - price_output_mae: 0.3612 - tier_output_accuracy: 0.8528 - tier_output_loss: 0.3619 - val_loss: 0.6478 - val_price_output_loss: 0.3581 - val_price_output_mae: 0.3849 - val_tier_output_accuracy: 0.7693 - val_tier_output_loss: 0.6300 - learning_rate: 2.5000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 0.3766 - price_output_loss: 0.3149 - price_output_mae: 0.3607 - tier_output_accuracy: 0.8530 - tier_output_loss: 0.3608 - val_loss: 0.6480 - val_price_output_loss: 0.3571 - val_price_output_mae: 0.3865 - val_tier_output_accuracy: 0.7694 - val_tier_output_loss: 0.6305 - learning_rate: 2.5000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - loss: 0.3720 - price_output_loss: 0.3138 - price_output_mae: 0.3602 - tier_output_accuracy: 0.8551 - tier_output_loss: 0.3562 - val_loss: 0.6533 - val_price_output_loss: 0.3499 - val_price_output_mae: 0.3825 - val_tier_output_accuracy: 0.7695 - val_tier_output_loss: 0.6359 - learning_rate: 2.5000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 6ms/step - loss: 0.3696 - price_output_loss: 0.3154 - price_output_mae: 0.3603 - tier_output_accuracy: 0.8556 - tier_output_loss: 0.3538 - val_loss: 0.6535 - val_price_output_loss: 0.3508 - val_price_output_mae: 0.3821 - val_tier_output_accuracy: 0.7696 - val_tier_output_loss: 0.6362 - learning_rate: 2.5000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 0.3689 - price_output_loss: 0.3136 - price_output_mae: 0.3598 - tier_output_accuracy: 0.8571 - tier_output_loss: 0.3533 - val_loss: 0.6537 - val_price_output_loss: 0.3537 - val_price_output_mae: 0.3830 - val_tier_output_accuracy: 0.7699 - val_tier_output_loss: 0.6362 - learning_rate: 2.5000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 6ms/step - loss: 0.3671 - price_output_loss: 0.3119 - price_output_mae: 0.3593 - tier_output_accuracy: 0.8561 - tier_output_loss: 0.3515 - val_loss: 0.6524 - val_price_output_loss: 0.3557 - val_price_output_mae: 0.3840 - val_tier_output_accuracy: 0.7704 - val_tier_output_loss: 0.6348 - learning_rate: 2.5000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 6ms/step - loss: 0.3658 - price_output_loss: 0.3114 - price_output_mae: 0.3588 - tier_output_accuracy: 0.8576 - tier_output_loss: 0.3501 - val_loss: 0.6554 - val_price_output_loss: 0.3527 - val_price_output_mae: 0.3813 - val_tier_output_accuracy: 0.7698 - val_tier_output_loss: 0.6379 - learning_rate: 2.5000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7ms/step - loss: 0.3650 - price_output_loss: 0.3136 - price_output_mae: 0.3588 - tier_output_accuracy: 0.8584 - tier_output_loss: 0.3493 - val_loss: 0.6580 - val_price_output_loss: 0.3542 - val_price_output_mae: 0.3832 - val_tier_output_accuracy: 0.7702 - val_tier_output_loss: 0.6403 - learning_rate: 2.5000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 6ms/step - loss: 0.3613 - price_output_loss: 0.3119 - price_output_mae: 0.3588 - tier_output_accuracy: 0.8584 - tier_output_loss: 0.3457 - val_loss: 0.6568 - val_price_output_loss: 0.3531 - val_price_output_mae: 0.3820 - val_tier_output_accuracy: 0.7692 - val_tier_output_loss: 0.6390 - learning_rate: 2.5000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - loss: 0.3595 - price_output_loss: 0.3101 - price_output_mae: 0.3580 - tier_output_accuracy: 0.8606 - tier_output_loss: 0.3439 - val_loss: 0.6570 - val_price_output_loss: 0.3558 - val_price_output_mae: 0.3840 - val_tier_output_accuracy: 0.7705 - val_tier_output_loss: 0.6391 - learning_rate: 2.5000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 6ms/step - loss: 0.3593 - price_output_loss: 0.3099 - price_output_mae: 0.3580 - tier_output_accuracy: 0.8596 - tier_output_loss: 0.3438 - val_loss: 0.6587 - val_price_output_loss: 0.3523 - val_price_output_mae: 0.3830 - val_tier_output_accuracy: 0.7697 - val_tier_output_loss: 0.6410 - learning_rate: 2.5000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m2977/2977\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - loss: 0.3577 - price_output_loss: 0.3081 - price_output_mae: 0.3569 - tier_output_accuracy: 0.8607 - tier_output_loss: 0.3422 - val_loss: 0.6622 - val_price_output_loss: 0.3568 - val_price_output_mae: 0.3849 - val_tier_output_accuracy: 0.7681 - val_tier_output_loss: 0.6443 - learning_rate: 2.5000e-04\n",
      "\n",
      "--- FINAL EVALUATION ---\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tier Accuracy: 0.7722\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.80      0.80     10666\n",
      "           1       0.69      0.70      0.69     10651\n",
      "           2       0.84      0.82      0.83     10430\n",
      "\n",
      "    accuracy                           0.77     31747\n",
      "   macro avg       0.77      0.77      0.77     31747\n",
      "weighted avg       0.77      0.77      0.77     31747\n",
      "\n",
      "Price MAE: ₹178.44\n"
     ]
    }
   ],
   "source": [
    "import os, re, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, classification_report\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import tensorflow as tf\n",
    "\n",
    "# Configuration\n",
    "DATA_PATH = \"indian_pharmaceutical_products_clean.csv\" \n",
    "RND = 42\n",
    "np.random.seed(RND)\n",
    "tf.random.set_seed(RND)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. LOAD & PREPROCESS (Standard)\n",
    "# ---------------------------------------------------------\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    print(f\"Error: File {DATA_PATH} not found.\")\n",
    "else:\n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    \n",
    "    # Cleaning\n",
    "    def normalize_text(s):\n",
    "        s = re.sub(r'[\\(\\)\\[\\]\\{\\},;:/\\\\\\|\"]', ' ', str(s).lower())\n",
    "        return re.sub(r'\\s+', ' ', s).strip()\n",
    "\n",
    "    def parse_strength(s):\n",
    "        m = re.search(r'(\\d+(?:\\.\\d+)?)\\s*(mcg|mg|g|µg|iu)?', str(s).lower())\n",
    "        if not m: return 0.0\n",
    "        v = float(m.group(1)); unit = (m.group(2) or '').replace('µg','mcg')\n",
    "        return v/1000.0 if unit == 'mcg' else (v*1000.0 if unit == 'g' else v)\n",
    "\n",
    "    def parse_pack(size, unit):\n",
    "        try:\n",
    "            if not pd.isna(size): return int(size)\n",
    "        except: pass\n",
    "        m = re.search(r'(\\d+)', str(unit).lower())\n",
    "        return int(m.group(1)) if m else 1\n",
    "\n",
    "    df['brand_clean'] = df['brand_name'].apply(normalize_text)\n",
    "    df['strength_mg'] = df['primary_strength'].apply(parse_strength)\n",
    "    df['pack_num'] = df.apply(lambda r: parse_pack(r.get('pack_size', pd.NA), r.get('pack_unit','')), axis=1)\n",
    "    df['composition_text'] = (df['primary_ingredient'].fillna('') + ' ' + df['active_ingredients'].fillna('')).apply(normalize_text)\n",
    "    df['text_for_emb'] = (df['brand_clean'] + ' ' + df['composition_text'] + ' ' + df['dosage_form'].fillna('').astype(str).str.lower())\n",
    "\n",
    "    df = df[df['price_inr'].notna()].reset_index(drop=True)\n",
    "    df['price_tier'] = pd.qcut(df['price_inr'], q=3, labels=[0,1,2]).astype(int)\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # 2. FEATURE ENGINEERING\n",
    "    # ---------------------------------------------------------\n",
    "    MAX_VOCAB = 10000; SEQ_LEN = 30; EMBED_DIM = 64\n",
    "    vectorizer = TextVectorization(max_tokens=MAX_VOCAB, output_sequence_length=SEQ_LEN)\n",
    "    vectorizer.adapt(df['text_for_emb'].astype(str).values)\n",
    "    \n",
    "    inp = layers.Input(shape=(1,), dtype=tf.string)\n",
    "    x = vectorizer(inp)\n",
    "    x = layers.Embedding(input_dim=len(vectorizer.get_vocabulary()), output_dim=EMBED_DIM, mask_zero=True)(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x) \n",
    "    enc = models.Model(inp, x)\n",
    "    emb_vectors = enc.predict(df['text_for_emb'].astype(str).values, batch_size=1024, verbose=0)\n",
    "    \n",
    "    df['embedding'] = list(emb_vectors)\n",
    "    EMB_DIM = emb_vectors.shape[1]\n",
    "\n",
    "    le_man = LabelEncoder()\n",
    "    df['manufacturer'] = df['manufacturer'].fillna('unknown').astype(str)\n",
    "    manu_counts = df['manufacturer'].value_counts()\n",
    "    df['manu_group'] = df['manufacturer'].apply(lambda x: x if manu_counts[x] > 10 else 'other')\n",
    "    df['manu_id'] = le_man.fit_transform(df['manu_group'])\n",
    "    \n",
    "    le_dos = LabelEncoder()\n",
    "    df['dosage_id'] = le_dos.fit_transform(df['dosage_form'].fillna('unknown').astype(str))\n",
    "    \n",
    "    features_numeric = ['pack_num', 'strength_mg']\n",
    "    num_scaler = StandardScaler()\n",
    "    df_scaled = df.copy()\n",
    "    df_scaled[features_numeric] = num_scaler.fit_transform(df_scaled[features_numeric])\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 3. SPLIT & PREPARE\n",
    "    # ---------------------------------------------------------\n",
    "    train_df, temp_df = train_test_split(df_scaled, test_size=0.25, random_state=RND, stratify=df_scaled['price_tier'])\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=RND, stratify=temp_df['price_tier'])\n",
    "\n",
    "    def make_inputs(d):\n",
    "        return {\n",
    "            'emb_input': np.vstack(d['embedding'].values).astype('float32'),\n",
    "            'manu_input': d['manu_id'].astype('int32').values,\n",
    "            'dosage_input': d['dosage_id'].astype('int32').values,\n",
    "            'num_input': d[features_numeric].astype('float32').values\n",
    "        }\n",
    "\n",
    "    X_train, X_val, X_test = make_inputs(train_df), make_inputs(val_df), make_inputs(test_df)\n",
    "\n",
    "    y_train_list = [train_df['price_tier'].values, np.log1p(train_df['price_inr'].values)]\n",
    "    y_val_list = [val_df['price_tier'].values, np.log1p(val_df['price_inr'].values)]\n",
    "    y_test_list = [test_df['price_tier'].values, np.log1p(test_df['price_inr'].values)]\n",
    "\n",
    "    # Weights\n",
    "    classes = np.unique(y_train_list[0])\n",
    "    cw = class_weight.compute_class_weight('balanced', classes=classes, y=y_train_list[0])\n",
    "    class_weight_dict = dict(enumerate(cw))\n",
    "    \n",
    "    sw_tier = np.array([class_weight_dict[y] for y in y_train_list[0]])\n",
    "    sw_price = np.ones(len(y_train_list[1]))\n",
    "    sample_weight_list = [sw_tier, sw_price]\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 4. ARCHITECTURE: \"Classification First\" (Restored Original Depth)\n",
    "    # ---------------------------------------------------------\n",
    "    MANU_VOCAB = df_scaled['manu_id'].nunique() + 1\n",
    "    DOSAGE_VOCAB = df_scaled['dosage_id'].nunique() + 1\n",
    "\n",
    "    # Inputs\n",
    "    emb_in = layers.Input(shape=(EMB_DIM,), name='emb_input')\n",
    "    manu_in = layers.Input(shape=(), dtype='int32', name='manu_input')\n",
    "    dosage_in = layers.Input(shape=(), dtype='int32', name='dosage_input')\n",
    "    num_in = layers.Input(shape=(len(features_numeric),), dtype='float32', name='num_input')\n",
    "\n",
    "    # Embeddings\n",
    "    manu_emb = layers.Flatten()(layers.Embedding(MANU_VOCAB, 32)(manu_in))\n",
    "    dos_emb = layers.Flatten()(layers.Embedding(DOSAGE_VOCAB, 8)(dosage_in))\n",
    "\n",
    "    # --- THE ORIGINAL DEEP SHARED STRUCTURE (This got you 94%) ---\n",
    "    # We bring back the full depth to ensure classification features are learned deeply\n",
    "    x = layers.Concatenate()([emb_in, manu_emb, dos_emb, num_in])\n",
    "    \n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    shared_features = layers.Dropout(0.2)(x)\n",
    "\n",
    "    # --- HEAD A: Tier (Direct connection) ---\n",
    "    tier_out = layers.Dense(3, activation='softmax', name='tier_output')(shared_features)\n",
    "\n",
    "    # --- HEAD B: Price (Side Branch) ---\n",
    "    # Small branch off the side. \n",
    "    price_branch = layers.Dense(32, activation='relu')(shared_features)\n",
    "    price_out = layers.Dense(1, activation='linear', name='price_output')(price_branch)\n",
    "\n",
    "    model = models.Model(inputs=[emb_in, manu_in, dosage_in, num_in], outputs=[tier_out, price_out])\n",
    "\n",
    "    # --- CRITICAL PART: LOSS WEIGHTING ---\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=1e-3),\n",
    "        loss={'tier_output': 'sparse_categorical_crossentropy', 'price_output': 'mse'},\n",
    "        \n",
    "        # STRATEGY: Focus 95% on Class, 5% on Price\n",
    "        # This prevents price gradients from ruining classification\n",
    "        loss_weights={'tier_output': 1.0, 'price_output': 0.05}, \n",
    "        \n",
    "        metrics={'tier_output': 'accuracy', 'price_output': 'mae'}\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 5. TRAINING\n",
    "    # ---------------------------------------------------------\n",
    "    class Monitor(callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            if epoch % 5 == 0: gc.collect()\n",
    "            \n",
    "    es = callbacks.EarlyStopping(monitor='val_tier_output_accuracy', mode='max', patience=10, restore_best_weights=True)\n",
    "    rlr = callbacks.ReduceLROnPlateau(monitor='val_tier_output_accuracy', mode='max', patience=4, factor=0.5, verbose=1)\n",
    "\n",
    "    print(\"Starting training (Classification First)...\")\n",
    "    history = model.fit(\n",
    "        x=X_train, y=y_train_list,\n",
    "        sample_weight=sample_weight_list,\n",
    "        validation_data=(X_val, y_val_list),\n",
    "        epochs=50,\n",
    "        batch_size=64, \n",
    "        callbacks=[es, rlr, Monitor()],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 6. RESULTS\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\n--- FINAL EVALUATION ---\")\n",
    "    preds = model.predict(X_test, batch_size=1024)\n",
    "    \n",
    "    # Tier\n",
    "    acc = accuracy_score(y_test_list[0], np.argmax(preds[0], axis=1))\n",
    "    print(f\"Tier Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_test_list[0], np.argmax(preds[0], axis=1)))\n",
    "    \n",
    "    # Price\n",
    "    pred_price = np.expm1(preds[1].flatten())\n",
    "    true_price = np.expm1(y_test_list[1])\n",
    "    mae = mean_absolute_error(true_price, pred_price)\n",
    "    print(f\"Price MAE: ₹{mae:.2f}\")\n",
    "    \n",
    "    model.save('final_multitask_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyspark_env)",
   "language": "python",
   "name": "pyspark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
